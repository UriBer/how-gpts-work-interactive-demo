import streamlit as st
import pandas as pd
import numpy as np
import torch  # PyTorch for tensor operations and model execution
from transformers import AutoTokenizer, AutoModel  # huggingface tools for loading tokenizer and model
import matplotlib.pyplot as plt  # for plotting the heatmap

# ×§×‘×™×¢×ª ×ª×¦×•×’×ª ×¢××•×“
title = "GPT Self-Attention Demo"
st.set_page_config(page_title=title, layout="centered")

# ×©×™××•×© ×‘×©×¤×” ××”-session ×”×§×•×“×
lang = st.session_state.get("lang", "×¢×‘×¨×™×ª")

# ×˜×•×¢×Ÿ ××•×“×œ ×•-tokenizer ××”×ª×™×§×™×™×” ×”××§×•××™×ª (× ×“×¨×© ×›×“×™ ×œ×× ×•×¢ ×”×•×¨×“×” ××”××™× ×˜×¨× ×˜)
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("./models/bert-base-multilingual-cased")
    model = AutoModel.from_pretrained(
        "./models/bert-base-multilingual-cased",
        output_attentions=True  # × ×‘×§×© ××”××•×“×œ ×œ×”×—×–×™×¨ ×’× ××ª ×©×›×‘×•×ª ×”×§×©×‘ (attention layers)
    )
    return tokenizer, model

tokenizer, model = load_model()

# ×©×™××•×© ×‘××©×¤×˜ ××”-session ×”×§×•×“× ××• ×”×’×“×¨×” ×œ×¤×™ ×‘×¨×™×¨×ª ××—×“×œ
user_sentence = st.session_state.get("user_sentence", "×”××”×¤×›×” ×©×œ GPT" if lang == "×¢×‘×¨×™×ª" else "The GPT revolution")

# Tokenize the sentence to convert it to tensor form for the model
# ×”×¤×™×›×ª ×”×˜×§×¡×˜ ×œ××–×”×™× ××¡×¤×¨×™×™× (×˜×•×§× ×™×) ×©×¢×•×‘×¨×™× ×œ××•×“×œ
inputs = tokenizer(user_sentence, return_tensors="pt", add_special_tokens=True)

# ×”×¨×¦×ª ×”××•×“×œ ×¢× ×”×§×œ×˜ ×•×—×™×œ×•×¥ ×©×›×‘×•×ª ×”×§×©×‘ (attention)
# torch.no_grad() ×—×•×¡×š ××©××‘×™× ×¢"×™ ×× ×™×¢×ª ×—×™×©×•×‘×™ ×’×¨×“×™×× ×˜
with torch.no_grad():
    outputs = model(**inputs)

# ×”××¨×ª ×”××–×”×™× ×”××¡×¤×¨×™×™× (input_ids) ×‘×—×–×¨×” ×œ××—×¨×•×–×•×ª ×˜×§×¡×˜
# ×œ×“×•×’××”: 101 â†’ [CLS], 102 â†’ [SEP]
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

# ×§×‘×œ×ª ××˜×¨×™×¦×ª ×”×§×©×‘ ××”×©×›×‘×” ×”××—×¨×•× ×”, head ××¡×¤×¨ 0
# outputs.attentions = list of tensors [layer][batch][head][token_from][token_to]
# ×›××Ÿ × ×‘×—×¨: layer ××—×¨×•×Ÿ, batch ×¨××©×•×Ÿ, head ×¨××©×•×Ÿ
attentions = outputs.attentions[-1]  # shape: [1, num_heads, seq_len, seq_len]
attn_matrix = attentions[0, 0].numpy()  # Convert tensor to NumPy matrix for visualization

# ×›×•×ª×¨×ª
st.subheader("\U0001F441\ufe0f Self-Attention" if lang == "English" else "\U0001F441\ufe0f ×× ×’× ×•×Ÿ ×”×§×©×‘ ×”×¢×¦××™ (Self-Attention)")

# ×”×¡×‘×¨ ×˜×§×¡×˜×•××œ×™ ×¢×œ ××˜×¨×™×¦×ª ×”×§×©×‘
st.markdown(
    """The chart below shows how much attention each word gives to the others.\nDarker colors = stronger influence.""" if lang == "English" else
    """×”×ª×¨×©×™× ×œ××˜×” ××¦×™×’ ×›××” ×›×œ ××™×œ×” ××ª×—×©×‘×ª ×‘×©××¨ ×”××™×œ×™× ×‘××©×¤×˜.\n×¦×‘×¢ ×›×”×” ×™×•×ª×¨ = ×”×©×¤×¢×” ×—×–×§×” ×™×•×ª×¨."""
)

# ×™×¦×™×¨×ª Heatmap ×¢× matplotlib
# ×”×¦×™×¨×™× ××¦×™×’×™× ××ª ×©××•×ª ×”×˜×•×§× ×™×, ×•×”×¦×‘×¢×™× ××¦×™×’×™× ××ª ×—×•×–×§ ×”×§×©×‘ ×‘×™× ×™×”×
fig, ax = plt.subplots()
cax = ax.matshow(attn_matrix, cmap='Blues')  # ××¦×™×™×¨ ××ª ××˜×¨×™×¦×ª ×”×§×©×‘ ×‘×¦×‘×¢×™×
fig.colorbar(cax)  # ××•×¡×™×£ ×¡×¨×’×œ ×¦×‘×¢×™×

# ×›×™×•×•× ×•×Ÿ ×”×¦×™×¨×™× ×œ×”×¦×’×ª ×©××•×ª ×”×˜×•×§× ×™×
ax.set_xticks(range(len(tokens)))
ax.set_yticks(range(len(tokens)))
ax.set_xticklabels(tokens, rotation=45)
ax.set_yticklabels(tokens)
ax.set_xlabel("Attention To" if lang == "English" else "×§×©×‘ ××œ")
ax.set_ylabel("From Token" if lang == "English" else "××”×˜×•×§×Ÿ")

# ×ª×¦×•×’×ª ×”×’×¨×£ ×‘××¤×œ×™×§×¦×™×”
st.pyplot(fig)

# ×¨××– ×œ××©×ª××© ×œ×”××©×™×š ×œ×©×œ×‘ ×”×‘×
st.info("â¬…ï¸ Next: Sentence completion (generation)" if lang == "English" else "â¬…ï¸ ×”×‘×: ×”×©×œ××ª ××©×¤×˜ ×¢× ×”××•×“×œ")

# ×›×¤×ª×•×¨ ××¢×‘×¨ ×™×“× ×™ ×œ××¡×š 3
if st.button("ğŸš€ Next: Text Generation" if lang == "English" else "ğŸš€ ×”××©×š ×œ×”×©×œ××ª ××©×¤×˜"):
    st.switch_page("pages/03-text-generation.py")
